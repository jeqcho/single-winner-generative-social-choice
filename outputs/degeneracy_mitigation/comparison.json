{
  "approach_a": {
    "minimal": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_a/minimal",
      "stats": {
        "approach": "A",
        "reasoning_effort": "minimal",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 20,
        "invalid_count": 80,
        "total_retries": 807,
        "voters_with_retries": 99,
        "retry_distribution": {
          "2": 5,
          "3": 5,
          "4": 3,
          "5": 8,
          "7": 8,
          "6": 14,
          "10": 12,
          "9": 8,
          "12": 11,
          "11": 8,
          "15": 3,
          "14": 3,
          "13": 2,
          "8": 8,
          "1": 1,
          "0": 1
        },
        "api_stats": {
          "count": 1307,
          "avg": 3.3197758423981085,
          "total": 4338.947026014328
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 79,
        "valid_rankings": 79
      }
    },
    "low": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_a/low",
      "stats": {
        "approach": "A",
        "reasoning_effort": "low",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 97,
        "invalid_count": 3,
        "total_retries": 67,
        "voters_with_retries": 40,
        "retry_distribution": {
          "0": 60,
          "1": 25,
          "3": 8,
          "2": 5,
          "4": 2
        },
        "api_stats": {
          "count": 567,
          "avg": 11.09492098885659,
          "total": 6290.820200681686
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 98,
        "valid_rankings": 98
      }
    },
    "medium": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_a/medium",
      "stats": {
        "approach": "A",
        "reasoning_effort": "medium",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 100,
        "invalid_count": 0,
        "total_retries": 5,
        "voters_with_retries": 4,
        "retry_distribution": {
          "0": 96,
          "2": 1,
          "1": 3
        },
        "api_stats": {
          "count": 505,
          "avg": 32.475452588808416,
          "total": 16400.10355734825
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 100,
        "valid_rankings": 100
      }
    }
  },
  "approach_b": {
    "minimal": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_b/minimal",
      "stats": {
        "approach": "B",
        "reasoning_effort": "minimal",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 72,
        "unresolved_duplicates_count": 28,
        "total_dedup_rounds": 78,
        "voters_needing_dedup": 43,
        "api_stats": {
          "count": 178,
          "avg": 8.862775422214122,
          "total": 1577.5740251541138
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 99,
        "valid_rankings": 99
      }
    },
    "low": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_b/low",
      "stats": {
        "approach": "B",
        "reasoning_effort": "low",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 100,
        "unresolved_duplicates_count": 0,
        "total_dedup_rounds": 0,
        "voters_needing_dedup": 0,
        "api_stats": {
          "count": 100,
          "avg": 15.953250107765198,
          "total": 1595.3250107765198
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 100,
        "valid_rankings": 100
      }
    },
    "medium": {
      "path": "/home/ec2-user/single-winner-generative-social-choice/outputs/degeneracy_mitigation/approach_b/medium",
      "stats": {
        "approach": "B",
        "reasoning_effort": "medium",
        "n_voters": 100,
        "n_statements": 100,
        "valid_count": 100,
        "unresolved_duplicates_count": 0,
        "total_dedup_rounds": 1,
        "voters_needing_dedup": 1,
        "api_stats": {
          "count": 101,
          "avg": 53.88587335548779,
          "total": 5442.473208904266
        }
      },
      "degeneracy": {
        "total": 100,
        "sequential_count": 0,
        "reverse_count": 0,
        "degenerate_count": 0,
        "degenerate_rate": 0.0,
        "unique_rankings": 98,
        "valid_rankings": 98
      }
    }
  },
  "correlations": {
    "minimal": {
      "mean_correlation": 0.48515822469718245,
      "std_correlation": 0.2998943644205393,
      "min_correlation": -0.5397059705970597,
      "max_correlation": 0.7998559855985596,
      "n_compared": 78
    },
    "low": {
      "mean_correlation": -0.011896128388349034,
      "std_correlation": 0.09823834441874021,
      "min_correlation": -0.25335733573357333,
      "max_correlation": 0.515067506750675,
      "n_compared": 98
    },
    "medium": {
      "mean_correlation": 0.01925421113539925,
      "std_correlation": 0.1435882109243048,
      "min_correlation": -0.2887248724872487,
      "max_correlation": 0.6427242724272426,
      "n_compared": 98
    }
  }
}